{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e2f7de",
   "metadata": {},
   "source": [
    "# AAE 722 — Lab 9 (SVM)\n",
    "\n",
    "**Author:** Wenshi (Gary) Sun  \n",
    "**Note:** Each code cell is clearly labeled with the question it answers. All figures are generated via matplotlib.\n",
    "\n",
    "---\n",
    "### Environment\n",
    "- Uses `scikit-learn`, `numpy`, `pandas`, `matplotlib`.  \n",
    "- For Hitters data I use `ISLP.load_data`. If `ISLP` is not installed, install it or replace with your local dataset source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from ISLP import load_data\n",
    "\n",
    "def plot_svm(model, X, y, title='', ax=None, h=0.02):\n",
    "    \"\"\"Generic decision-boundary plotter for 2D features.\n",
    "    - model: fitted sklearn SVM\n",
    "    - X: (n,2) array\n",
    "    - y: binary labels (0/1 or 1/2)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5,4))\n",
    "    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    cmap_light = ListedColormap(['#E0F3FF','#FFE5E0'])\n",
    "    cmap_bold  = ['#1f77b4','#ff7f0e']\n",
    "    ax.contourf(xx, yy, Z, alpha=0.35, cmap=cmap_light)\n",
    "    classes = np.unique(y)\n",
    "    for i, cls in enumerate(classes):\n",
    "        ax.scatter(X[y==cls,0], X[y==cls,1], s=25, edgecolor='k', label=str(cls), c=cmap_bold[i])\n",
    "    if hasattr(model, 'support_vectors_'):\n",
    "        ax.scatter(model.support_vectors_[:,0], model.support_vectors_[:,1], s=80, facecolors='none', edgecolors='k', linewidths=1.2, label='SV')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='best')\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ad395",
   "metadata": {},
   "source": [
    "## Question 1 — Linear SVM and Cost Parameter Effect (0.175 pts)\n",
    "**Task.** Use Hitters, drop missing values, create `High = 1(Salary > median)`. Using only `CHits` and `CWalks` as features, fit two linear SVMs with `C=0.1` and `C=100`. Report **number of support vectors** (use `len(svc.support_)`) and show **decision-boundary plots** (both labeled). Provide a **3–4 sentence** comparison of how `C` affects margin width, support-vector count, and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed73f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 — data prep & two linear SVMs\n",
    "Hitters = load_data('Hitters').dropna()\n",
    "y = (Hitters['Salary'] > Hitters['Salary'].median()).astype(int)\n",
    "X = Hitters[['CHits','CWalks']].to_numpy()\n",
    "pipe_C01 = make_pipeline(StandardScaler(), SVC(kernel='linear', C=0.1, random_state=42))\n",
    "pipe_C100 = make_pipeline(StandardScaler(), SVC(kernel='linear', C=100, random_state=42))\n",
    "pipe_C01.fit(X, y)\n",
    "pipe_C100.fit(X, y)\n",
    "svc_C01 = pipe_C01.named_steps['svc']\n",
    "svc_C100 = pipe_C100.named_steps['svc']\n",
    "print('Q1 — #Support Vectors (C=0.1):', len(svc_C01.support_))\n",
    "print('Q1 — #Support Vectors (C=100):', len(svc_C100.support_))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11,4))\n",
    "plot_svm(svc_C01, pipe_C01.named_steps['standardscaler'].transform(X), y, title='Linear SVM (C=0.1, scaled)', ax=axes[0])\n",
    "plot_svm(svc_C100, pipe_C100.named_steps['standardscaler'].transform(X), y, title='Linear SVM (C=100, scaled)', ax=axes[1])\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d125d",
   "metadata": {},
   "source": [
    "**Short Discussion (Q1).**  \n",
    "With **smaller C (0.1)**, the classifier accepts more margin violations to obtain a **wider margin**, which typically **increases the number of support vectors** and yields a **smoother/less flexible** boundary.  \n",
    "With **larger C (100)**, the optimization penalizes violations strongly, producing a **narrower margin** and often **fewer support vectors**; the boundary becomes **more flexible** and may overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a00f12",
   "metadata": {},
   "source": [
    "## Question 2 — Hyperparameter Tuning with Cross‑Validation (0.15 pts)\n",
    "**Task.** Same Hitters features. Do 5‑fold `GridSearchCV` over `C = [0.01, 0.1, 1, 10, 100]` for a linear SVM. Report **best C**, **best CV accuracy**, and **all mean CV scores**. Create a **validation‑curve plot** with log‑scale x‑axis and mark the best C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 — 5-fold CV grid over C\n",
    "Cs = [0.01, 0.1, 1, 10, 100]\n",
    "pipe = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=42))\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(pipe, {'svc__C': Cs}, cv=cv, scoring='accuracy', refit=True)\n",
    "grid.fit(X, y)\n",
    "best_C = grid.best_params_['svc__C']\n",
    "best_cv = grid.best_score_\n",
    "print('Q2 — Best C:', best_C)\n",
    "print('Q2 — Best CV accuracy:', round(best_cv, 4))\n",
    "means = [grid.cv_results_['mean_test_score'][i] for i in range(len(Cs))]\n",
    "res = pd.DataFrame({'C': Cs, 'mean_cv_accuracy': means})\n",
    "display(res)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.semilogx(Cs, means, marker='o')\n",
    "plt.xlabel('C (log scale)'); plt.ylabel('Mean CV Accuracy')\n",
    "plt.title('Validation Curve — Linear SVM (CHits, CWalks)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.scatter([best_C], [grid.best_score_], s=80, facecolors='none', edgecolors='r', label=f'Best C={best_C}')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa008ded",
   "metadata": {},
   "source": [
    "## Question 3 — RBF Kernel and Model Comparison (0.175 pts)\n",
    "**Task.** Generate data, split 50/50, fit RBF SVM (`C=1, gamma=1`), report train/test accuracy and plot the boundary. Then fit `gamma ∈ {0.1,1,10}` and plot **ROC curves** for all three on one figure; explain briefly which gamma is best and how `gamma` controls complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 — synthetic data and RBF comparisons\n",
    "np.random.seed(42)\n",
    "Xsyn = np.random.randn(200, 2)\n",
    "Xsyn[:100] += 2\n",
    "Xsyn[100:150] -= 2\n",
    "y_syn = np.array([1]*150 + [2]*50)\n",
    "y_syn01 = (y_syn == 2).astype(int)\n",
    "Xtr_s, Xte_s, ytr_s, yte_s, ytr01, yte01 = train_test_split(\n",
    "    Xsyn, y_syn, y_syn01, test_size=0.5, random_state=42, stratify=y_syn\n",
    ")\n",
    "rbf1 = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1, gamma=1, probability=True, random_state=42))\n",
    "rbf1.fit(Xtr_s, ytr_s)\n",
    "acc_tr = accuracy_score(ytr_s, rbf1.predict(Xtr_s))\n",
    "acc_te = accuracy_score(yte_s, rbf1.predict(Xte_s))\n",
    "print('Q3 — RBF gamma=1: Train acc =', round(acc_tr,3), ', Test acc =', round(acc_te,3))\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "Xtr_scaled = rbf1.named_steps['standardscaler'].transform(Xtr_s)\n",
    "plot_svm(rbf1.named_steps['svc'], Xtr_scaled, (ytr_s==2).astype(int), title='RBF SVM (gamma=1) — train (scaled)', ax=ax)\n",
    "plt.show()\n",
    "\n",
    "gammas = [0.1, 1, 10]\n",
    "plt.figure(figsize=(6,5))\n",
    "for g in gammas:\n",
    "    pipe = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1, gamma=g, probability=True, random_state=42))\n",
    "    pipe.fit(Xtr_s, ytr_s)\n",
    "    proba = pipe.predict_proba(Xte_s)[:,1]\n",
    "    fpr, tpr, _ = roc_curve((yte_s==2).astype(int), proba)\n",
    "    from sklearn.metrics import auc\n",
    "    AUC = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'gamma={g} (AUC={AUC:.3f})')\n",
    "plt.plot([0,1],[0,1],'k--', lw=1)\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves — RBF SVM on Synthetic Data')\n",
    "plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c46188",
   "metadata": {},
   "source": [
    "**Short Interpretation (Q3).**  \n",
    "Typically, **gamma≈1** gives the best balance on this dataset: it captures the non‑linear boundary without overfitting, yielding the highest AUC/accuracy among the three.  \n",
    "A **smaller gamma (0.1)** yields smoother, high‑bias boundaries that may **underfit**. A **larger gamma (10)** produces highly wiggly boundaries with **high variance/overfitting**, hurting test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050a423",
   "metadata": {},
   "source": [
    "---\n",
    "### Submission Checklist (Lab 9)\n",
    "- [ ] Run all cells so outputs and plots are visible.\n",
    "- [ ] Save the notebook after execution.\n",
    "- [ ] Push to your AAE 722 GitHub repo (shared with Pragya & Jing).\n",
    "- [ ] Share the **direct link** to this Lab 9 notebook on Canvas and verify the link opens the notebook file directly.\n",
    "- [ ] Ensure each question is clearly labeled and includes the requested plots and short discussion."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
