{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a64ec7b",
   "metadata": {},
   "source": [
    "# AAE 722 — Lab 8 (Supervised Learning)\n",
    "\n",
    "**Author:** Wenshi (Gary) Sun  \n",
    "**Note:** Each question cell is clearly labeled. I re-ran all code locally so outputs are visible.\n",
    "\n",
    "---\n",
    "### Environment & Notes\n",
    "- Using `ISLP` datasets (`Hitters`, `OJ`, `Auto`).\n",
    "- Random seeds fixed at `random_state=42`.\n",
    "- Metrics rounded per question instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC, DecisionTreeRegressor as DTR\n",
    "from sklearn.ensemble import RandomForestRegressor as RF, GradientBoostingClassifier as GBC\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((np.array(y_true) - np.array(y_pred))**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab32d0",
   "metadata": {},
   "source": [
    "## Question 1 — Bagging for Salary on Hitters\n",
    "**Task.** Remove rows with missing *Salary* values, split 70/30 (random_state=42), fit a bagging model (RF with `max_features = #predictors`) using 300 trees to predict `Salary`. Report test MSE (2 decimals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "Hitters = load_data('Hitters')\n",
    "Hitters = Hitters.dropna(subset=['Salary'])\n",
    "spec_q1 = MS(Hitters.columns.drop('Salary'), intercept=False)\n",
    "D_q1 = spec_q1.fit_transform(Hitters)\n",
    "X_q1 = np.asarray(D_q1)\n",
    "y_q1 = Hitters['Salary'].to_numpy()\n",
    "(Xtr_q1, Xte_q1, ytr_q1, yte_q1) = skm.train_test_split(\n",
    "    X_q1, y_q1, test_size=0.30, random_state=42\n",
    ")\n",
    "bag_model = RF(\n",
    "    n_estimators=300,\n",
    "    max_features=Xtr_q1.shape[1],\n",
    "    random_state=42\n",
    ")\n",
    "bag_model.fit(Xtr_q1, ytr_q1)\n",
    "yhat_q1 = bag_model.predict(Xte_q1)\n",
    "mse_q1 = mse(yte_q1, yhat_q1)\n",
    "print('Q1 — Test MSE:', round(mse_q1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26dd860",
   "metadata": {},
   "source": [
    "## Question 2 — Random Forest with `max_features=5` (same split as Q1)\n",
    "**Task.** Using the same train/test split from Q1, fit RF with `max_features=5`, 300 trees. Show a DataFrame of feature importances (descending). Identify the top variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc60608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "rf5 = RF(n_estimators=300, max_features=5, random_state=42)\n",
    "rf5.fit(Xtr_q1, ytr_q1)\n",
    "imp_q2 = (pd.DataFrame({'feature': D_q1.columns,\n",
    "                        'importance': rf5.feature_importances_})\n",
    "          .sort_values('importance', ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "display(imp_q2.head(10))\n",
    "top_var_q2 = imp_q2.loc[0, 'feature']\n",
    "print('Q2 — Most important variable:', top_var_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ec82f",
   "metadata": {},
   "source": [
    "## Question 3 — Decision Tree Classifier on OJ\n",
    "**Task.** Predict `Purchase` using all other variables. Split 75/25 (random_state=42). Fit `DTC(criterion=\"entropy\", max_depth=4, min_samples_leaf=5, random_state=42)`. Report test accuracy (3 decimals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707444dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "OJ = load_data('OJ')\n",
    "spec_q3 = MS(OJ.columns.drop('Purchase'), intercept=False)\n",
    "D_q3 = spec_q3.fit_transform(OJ)\n",
    "X_q3 = np.asarray(D_q3)\n",
    "y_q3 = OJ['Purchase'].to_numpy()\n",
    "(Xtr_q3, Xte_q3, ytr_q3, yte_q3) = skm.train_test_split(\n",
    "    X_q3, y_q3, test_size=0.25, random_state=42\n",
    ")\n",
    "dtc = DTC(criterion='entropy', max_depth=4, min_samples_leaf=5, random_state=42)\n",
    "dtc.fit(Xtr_q3, ytr_q3)\n",
    "acc_q3 = accuracy_score(yte_q3, dtc.predict(Xte_q3))\n",
    "print('Q3 — Test accuracy:', round(acc_q3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353591a",
   "metadata": {},
   "source": [
    "## Question 4 — Gradient Boosting on Auto (binary mpg_high)\n",
    "**Task.** Remove missing rows. Create `mpg_high = 1(mpg > median)`. Split 70/30 (random_state=42). Use all variables except `mpg` and `name`. Fit `GBC(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)`. Report test accuracy (3 decimals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce568b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "Auto = load_data('Auto').dropna()\n",
    "Auto = Auto.copy()\n",
    "Auto['mpg_high'] = (Auto['mpg'] > Auto['mpg'].median()).astype(int)\n",
    "predictors_q4 = [c for c in Auto.columns if c not in ['mpg', 'mpg_high', 'name']]\n",
    "spec_q4 = MS(predictors_q4, intercept=False)\n",
    "D_q4 = spec_q4.fit_transform(Auto)\n",
    "X_q4 = np.asarray(D_q4)\n",
    "y_q4 = Auto['mpg_high'].to_numpy()\n",
    "(Xtr_q4, Xte_q4, ytr_q4, yte_q4) = skm.train_test_split(\n",
    "    X_q4, y_q4, test_size=0.30, random_state=42\n",
    ")\n",
    "gbc = GBC(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbc.fit(Xtr_q4, ytr_q4)\n",
    "acc_q4 = accuracy_score(yte_q4, gbc.predict(Xte_q4))\n",
    "print('Q4 — Test accuracy:', round(acc_q4, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f0fe8",
   "metadata": {},
   "source": [
    "## Question 5 — Regression Tree with Cost-Complexity Pruning (Hitters)\n",
    "**Task.** On `Hitters` (same NA rule), split 70/30 (random_state=42). Fit `DTR(max_depth=6, random_state=42)`. Use `cost_complexity_pruning_path` to build a grid over `ccp_alpha`, then `GridSearchCV(cv=5, shuffle=True, random_state=42)` with scoring=`neg_mean_squared_error`. Report: (a) number of leaf nodes of the best estimator; (b) test MSE of the pruned tree (2 decimals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "Hitters2 = load_data('Hitters')\n",
    "Hitters2 = Hitters2.dropna(subset=['Salary'])\n",
    "spec_q5 = MS(Hitters2.columns.drop('Salary'), intercept=False)\n",
    "D_q5 = spec_q5.fit_transform(Hitters2)\n",
    "X_q5 = np.asarray(D_q5)\n",
    "y_q5 = Hitters2['Salary'].to_numpy()\n",
    "(Xtr_q5, Xte_q5, ytr_q5, yte_q5) = skm.train_test_split(\n",
    "    X_q5, y_q5, test_size=0.30, random_state=42\n",
    ")\n",
    "tree0 = DTR(max_depth=6, random_state=42)\n",
    "tree0.fit(Xtr_q5, ytr_q5)\n",
    "path = tree0.cost_complexity_pruning_path(Xtr_q5, ytr_q5)\n",
    "alphas = np.unique(path.ccp_alphas)\n",
    "cv = skm.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gs = skm.GridSearchCV(\n",
    "    estimator=DTR(random_state=42),\n",
    "    param_grid={'ccp_alpha': alphas},\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=cv,\n",
    "    refit=True\n",
    ")\n",
    "gs.fit(Xtr_q5, ytr_q5)\n",
    "best_tree = gs.best_estimator_\n",
    "n_leaves = best_tree.tree_.n_leaves\n",
    "yhat_q5 = best_tree.predict(Xte_q5)\n",
    "mse_q5 = mse(yte_q5, yhat_q5)\n",
    "print('Q5 — Best ccp_alpha:', gs.best_params_['ccp_alpha'])\n",
    "print('Q5 — #Leaf nodes:', n_leaves)\n",
    "print('Q5 — Test MSE (pruned):', round(mse_q5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a500f",
   "metadata": {},
   "source": [
    "---\n",
    "### Submission Checklist\n",
    "- [ ] All cells executed and outputs visible.\n",
    "- [ ] Notebook saved.\n",
    "- [ ] Pushed to GitHub (AAE722 repo) in a clear path (e.g., `Labs/Lab8/AAE722_Lab8_WenshiSun.ipynb`).\n",
    "- [ ] Direct link shared on Canvas (opens the notebook file directly).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
